%!TEX root = InfoSec.tex
% Lecture 9: 8 October 2014
\sektion{9}{Secure Programming}

Information flow: how to control propagation of information within a program or
between programs on a system where there is some confidentiality requirement.

Consider a program $P(v, s, r)$:
\begin{itemize}
    \item $v$: visible (public) input
    \item $s$: secret input
    \item $r$: randomness (secret)
\end{itemize}
Output: all visible actions of program but doesn't leak secret input

Does the output of $P$ leak information about $s$? Define a game against
adversary where he provides $s_0$ and $s_1$. We announce $P(v, s_b, r)$ where $b \in \{1, 0\}$ and $r$ is secret and random. The adversary guesses what $b$ is. Security is defined by the adversary having no strategy that nets him a a non-negligible advantage in finding $b$.

How to enforce non-leakiness?

Unlike with previous properties, cannot enforce by watching $P$ run.
(Just because no output came out doesn't mean there wasn't a leak - ``dog that
didn't bark problem''). It's inherently necessary to consider what-ifs that
differ from what you actually saw

\subsektion{Lattice model}
General model for information flow policy

\begin{definition}{Lattice} % these are very similiar to binary relations in econ310

    $(S, \sqsubseteq)$, $S$: set of labels, $\sqsubseteq$: partial order such
    that for any $a, b \in S$, there exists a least upper bound $u \in S$ and a
    greatest lower bound $l \in S$.

    least upper bound of $a, b$:
    \begin{itemize}
        \item $a \sqsubseteq u$ and $b \sqsubseteq u$ and for all $v \in S$,
            $a \sqsubseteq v$ and $b \sqsubseteq v$ $\Rightarrow$
            $u \sqsubseteq v$
    \end{itemize}

    partial order:
    \begin{itemize}
        \item reflexive: $a \sqsubseteq a$
        \item transitive: $a \sqsubseteq b$ and $b \sqsubseteq c$, then
            $a \sqsubseteq c$
        \item asymmetric: $a \sqsubseteq b$ and $b \sqsubseteq a$, then $a = b$
    \end{itemize}

\end{definition}
\begin{example}{Lattices}
    \begin{enumerate}
        \item linear chain of labels:

            public $\sqsubseteq$ confidential

            unclassified $\sqsubseteq$ classified $\sqsubseteq$ secret
                $\sqsubseteq$ top secret
        \item compartments (eg. project, client ID, job function)

            label (project) is set of states (project 1, project 2, etc.), $\sqsubseteq$ is subset
        \item org chart

            label is node in chart, $\sqsubseteq$ is ancestor/descendant
        \item combination/cross product of lattices

            label is $(S_1, S_2)$, $(A_1, B_1) \sqsubseteq (A_2, B_2)$ iff
            $A_1 \sqsubseteq A_2$ and $B_1 \sqsubseteq B_2$
    \end{enumerate}
\end{example}
\subsektion{Enforcing flow in a program}
At each point in the program, every variable has a label (that comes from the
lattice we're using). Inputs are tagged with label, outputs are tagged with a requirement. Labels are propagated
when code executes:
$$a = b \Rightarrow \text{label}(a) = \text{label}(b)$$
$$a = b + c \Rightarrow \text{label}(a) = \text{LUB}(\text{label}(b), \text{label}(c))$$
Where LUB = Least Upper Bound. ``Go to a place that's at least as well protected as b and at least as well protected as c.''

Before providing output, check that state of output value is consistent with
policy. Allow the output of a value $v$ where $L$ is required if and only if label($v$) $\sqsubseteq L$

But this isn't enough (only monitoring and rejecting when inconsistent with
policy) -- ``dog that didn't bark''. \textbf{A troublesome example:}
\begin{verbatim}
// label(a) = ``secret''
b = 0;  // 0 isn't secret, so b is set to unclassified
if (a > 5)  b = 1;  // Requires static (compile-time) analysis to get right
output b;   // Says something about a, so should be labelled as secret
\end{verbatim}
Static analysis won't catch all, but will catch some of the leaks.
\begin{description}
    \item[Problem 1:] conservative analysis leads to being overly cautious
    \item[Problem 2:] timing might depend on values
    \item[Problem 3:] ``covert channels''; shared resource channels might reveal clues about, for example, what files are being opened and what secret(s) that might affect.
\end{description}

{\bf What if you can't prevent a program from leaking the information it has?}

{\bf Bell-LaPadula model}: lattice-based information flow for programs and files
\begin{itemize}
\item every program has a label (from lattice): what it's allowed to access
\item every file has a label: what it contains
\item Rule 1: ``No Read Up'' - Program $P$ can read File $F$ only if label($F$)
    $\sqsubseteq$ label($P$)
\item Rule 2: ``No Write Down'' - Program $P$ can write File $F$ only if
    label($P$) $\sqsubseteq$ label($F$); Programs can't talk to ``lower'' files so as
    not to leak information
\end{itemize}
\begin{theorem*} If label($F_1$) $\sqsubseteq$ label($F_2$) and the two rules are
enforced, then information from $F_2$ cannot leak into $F_1$.
\end{theorem*}

Problems:
\begin{enumerate}
    \item expections (need to make explicit loopholes in system to allow)
    \begin{itemize}
        \item declassification of old data
        \item aggregate/``anonymized'' data
        \item policy decision to make exception
    \end{itemize}
    \item usability - system can't tell you if there are classified files in a
        directory you're trying to delete or no space on disk for you to add a
        file
    \item outside channels - people talk to each other outside the system
\end{enumerate}

This, so far, has been about confidentiality. Can we do the same thing for
integrity?

\begin{example}
Any program is able to delete a file and overwrite ``secret plans''. Our program/file
is public, so it can write UP and replace contents of ``secret plans''. This is bad.
\end{example}

{\bf Biba model}: (B-LP for integrity)
\begin{itemize}
    \item Rule 1: ``No Read Down''
    \item Rule 2: ``No Write Up''
\end{itemize}

B-LP model and Biba model at the same time?
\begin{itemize}
    \item if use same labels for both (high confidentiality = high integrity),
        then no communication between levels
    \item if different labels, then some information flows become possible, but
        could result in being much more difficult for users
    \item especially with static program analysis, things become conservative
        and flexibility is lost
    \item result: usually focus on confidentiality or integrity and let humans
        worry about this outside of the system
\end{itemize}

The takeaway is that information flow tools are useful for preventing yourself from
making mistakes, but not so useful to protect against an adversary.

\sidenote{{\bf Back to crypto... [a note from 2012]}

Secret sharing:
\begin{itemize}
    \item divide a secret into ``shares'' so that all share are required to
        reconstruct secret
        \begin{itemize}
            \item 2-way: pick a large value $M$, secret is some $s$,
                $0 \le s < M$\\
                pick $r$ randomly, $0 \le r < M$\\
                shares are $r$, $(s-r) \mod M$\\
                to reconstruct, add shares $\mod M$
            \item $k$-way: shares $r_0, r_1, \dots, r_{k-2}$ random,
                $(s - (r_0 + \cdots + r_{l-2})) \mod M$
            \item can also construct degree $k$ polynomials such that $k$ values
                are needed to reconstruct
        \end{itemize}
    \item suppose RSA private key is $(d, N)$, shares
        $(d_1, N), (d_2, N), (d_3, N)$ such that
        $d_1 + d_2 + d_3 = d \mod(p-1)(q-1)$

        $\left(X^{d_1} \mod N\right)\left(X^{d_2}\right)\left(X^{d_3}\right) =
        X^{(d_1 + d_2 + d_3)\mod(p-1)(q-1)} \mod N = X^d \mod N$

        (splits up an RSA operation)
\end{itemize}
}
